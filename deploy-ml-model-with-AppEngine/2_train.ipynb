{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Training on Cloud AI Platform</h1>\n",
    "\n",
    "This notebook illustrates distributed training on Cloud AI Platform (formerly known as Cloud ML Engine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo chown -R jupyter:jupyter /home/jupyter/GCP-trainings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the right version of Tensorflow is installed.\n",
    "!pip freeze | grep tensorflow==2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change these to try this notebook out\n",
    "BUCKET = 'trainings-ml-deployment-appengine-xx'\n",
    "PROJECT = 'rare-result-248415'\n",
    "REGION = 'europe-west1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['TFVERSION'] = '2.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://trainings-ml-deployment-appengine-ag/babyweight/preproc/eval.csv-00000-of-00008\n",
      "gs://trainings-ml-deployment-appengine-ag/babyweight/preproc/train.csv-00000-of-00015\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil ls gs://${BUCKET}/babyweight/preproc/*-00000*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the TensorFlow code working on a subset of the data, we can package the TensorFlow code up as a Python module and train it on Cloud AI Platform.\n",
    "<p>\n",
    "<h2> Train on Cloud AI Platform</h2>\n",
    "<p>\n",
    "Training on Cloud AI Platform requires:\n",
    "<ol>\n",
    "<li> Making the code a Python package\n",
    "<li> Using gcloud to submit the training code to Cloud AI Platform\n",
    "</ol>\n",
    "\n",
    "Ensure that the AI Platform API is enabled by going to this [link](https://console.developers.google.com/apis/library/ml.googleapis.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Task 1\n",
    "\n",
    "The following code edits babyweight/trainer/task.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model/trainer/task.py\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "from . import model\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--bucket',\n",
    "        help = 'GCS path to data. We assume that data is in gs://BUCKET/babyweight/preproc/',\n",
    "        required = True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--output_dir',\n",
    "        help = 'GCS location to write checkpoints and export models',\n",
    "        required = True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--batch_size',\n",
    "        help = 'Number of examples to compute gradient over.',\n",
    "        type = int,\n",
    "        default = 512\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--job-dir',\n",
    "        help = 'this model ignores this field, but it is required by gcloud',\n",
    "        default = 'junk'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--nnsize',\n",
    "        help = 'Hidden layer sizes to use for DNN feature columns -- provide space-separated layers',\n",
    "        nargs = '+',\n",
    "        type = int,\n",
    "        default=[128, 32, 4]\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--nembeds',\n",
    "        help = 'Embedding size of a cross of n key real-valued parameters',\n",
    "        type = int,\n",
    "        default = 3\n",
    "    )\n",
    "\n",
    "    ## TODO 1: add the new arguments here \n",
    "    parser.add_argument(\n",
    "        '--train_examples',\n",
    "        help = 'Number of examples (in thousands) to run the training job over. If this is more than actual # of examples available, it cycles through them. So specifying 1000 here when you have only 100k examples makes this 10 epochs.',\n",
    "        type = int,\n",
    "        default = 5000\n",
    "    )    \n",
    "    parser.add_argument(\n",
    "        '--pattern',\n",
    "        help = 'Specify a pattern that has to be in input files. For example 00001-of will process only one shard',\n",
    "        default = 'of'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--eval_steps',\n",
    "        help = 'Positive number of steps for which to evaluate model. Default to None, which means to evaluate until input_fn raises an end-of-input exception',\n",
    "        type = int,       \n",
    "        default = None\n",
    "    )\n",
    "        \n",
    "    ## parse all arguments\n",
    "    args = parser.parse_args()\n",
    "    arguments = args.__dict__\n",
    "\n",
    "    # unused args provided by service\n",
    "    arguments.pop('job_dir', None)\n",
    "    arguments.pop('job-dir', None)\n",
    "\n",
    "    ## assign the arguments to the model variables\n",
    "    output_dir = arguments.pop('output_dir')\n",
    "    model.BUCKET     = arguments.pop('bucket')\n",
    "    model.BATCH_SIZE = arguments.pop('batch_size')\n",
    "    model.TRAIN_STEPS = (arguments.pop('train_examples') * 100) / model.BATCH_SIZE\n",
    "    model.EVAL_STEPS = arguments.pop('eval_steps')    \n",
    "    print (\"Will train for {} steps using batch_size={}\".format(model.TRAIN_STEPS, model.BATCH_SIZE))\n",
    "    model.PATTERN = arguments.pop('pattern')\n",
    "    model.NEMBEDS= arguments.pop('nembeds')\n",
    "    model.NNSIZE = arguments.pop('nnsize')\n",
    "    print (\"Will use DNN size of {}\".format(model.NNSIZE))\n",
    "\n",
    "    # Append trial_id to path if we are doing hptuning\n",
    "    # This code can be removed if you are not using hyperparameter tuning\n",
    "    output_dir = os.path.join(\n",
    "        output_dir,\n",
    "        json.loads(\n",
    "            os.environ.get('TF_CONFIG', '{}')\n",
    "        ).get('task', {}).get('trial', '')\n",
    "    )\n",
    "\n",
    "    # Run the training job\n",
    "    model.train_and_evaluate(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Task 2\n",
    "\n",
    "The following code edits babyweight/trainer/model.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model/trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model/trainer/model.py\n",
    "import shutil\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "BUCKET = None  # set from task.py\n",
    "PATTERN = 'of' # gets all files\n",
    "\n",
    "# Determine CSV, label, and key columns\n",
    "CSV_COLUMNS = 'weight_pounds,is_male,mother_age,plurality,gestation_weeks,key'.split(',')\n",
    "LABEL_COLUMN = 'weight_pounds'\n",
    "KEY_COLUMN = 'key'\n",
    "\n",
    "# Set default values for each CSV column\n",
    "DEFAULTS = [[0.0], ['null'], [0.0], ['null'], [0.0], ['nokey']]\n",
    "\n",
    "# Define some hyperparameters\n",
    "TRAIN_STEPS = 10000\n",
    "EVAL_STEPS = None\n",
    "BATCH_SIZE = 512\n",
    "NEMBEDS = 3\n",
    "NNSIZE = [64, 16, 4]\n",
    "\n",
    "# Create an input function reading a file using the Dataset API\n",
    "# Then provide the results to the Estimator API\n",
    "def read_dataset(prefix, mode, batch_size):\n",
    "    def _input_fn():\n",
    "        def decode_csv(value_column):\n",
    "            columns = tf.decode_csv(value_column, record_defaults=DEFAULTS)\n",
    "            features = dict(zip(CSV_COLUMNS, columns))\n",
    "            label = features.pop(LABEL_COLUMN)\n",
    "            return features, label\n",
    "        \n",
    "        # Use prefix to create file path\n",
    "        file_path = 'gs://{}/babyweight/preproc/{}*{}*'.format(BUCKET, prefix, PATTERN)\n",
    "\n",
    "        # Create list of files that match pattern\n",
    "        file_list = tf.gfile.Glob(file_path)\n",
    "\n",
    "        # Create dataset from file list\n",
    "        dataset = (tf.data.TextLineDataset(file_list)  # Read text file\n",
    "                    .map(decode_csv))  # Transform each elem by applying decode_csv fn\n",
    "      \n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            num_epochs = None # indefinitely\n",
    "            dataset = dataset.shuffle(buffer_size = 10 * batch_size)\n",
    "        else:\n",
    "            num_epochs = 1 # end-of-input after this\n",
    " \n",
    "        dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "        return dataset.make_one_shot_iterator().get_next()\n",
    "    return _input_fn\n",
    "\n",
    "# Define feature columns\n",
    "def get_wide_deep():\n",
    "    # Define column types\n",
    "    is_male,mother_age,plurality,gestation_weeks = \\\n",
    "        [\\\n",
    "            tf.feature_column.categorical_column_with_vocabulary_list('is_male', \n",
    "                        ['True', 'False', 'Unknown']),\n",
    "            tf.feature_column.numeric_column('mother_age'),\n",
    "            tf.feature_column.categorical_column_with_vocabulary_list('plurality',\n",
    "                        ['Single(1)', 'Twins(2)', 'Triplets(3)',\n",
    "                         'Quadruplets(4)', 'Quintuplets(5)','Multiple(2+)']),\n",
    "            tf.feature_column.numeric_column('gestation_weeks')\n",
    "        ]\n",
    "\n",
    "    # Discretize\n",
    "    age_buckets = tf.feature_column.bucketized_column(mother_age, \n",
    "                        boundaries=np.arange(15,45,1).tolist())\n",
    "    gestation_buckets = tf.feature_column.bucketized_column(gestation_weeks, \n",
    "                        boundaries=np.arange(17,47,1).tolist())\n",
    "      \n",
    "    # Sparse columns are wide, have a linear relationship with the output\n",
    "    wide = [is_male,\n",
    "            plurality,\n",
    "            age_buckets,\n",
    "            gestation_buckets]\n",
    "    \n",
    "    # Feature cross all the wide columns and embed into a lower dimension\n",
    "    crossed = tf.feature_column.crossed_column(wide, hash_bucket_size=20000)\n",
    "    embed = tf.feature_column.embedding_column(crossed, NEMBEDS)\n",
    "    \n",
    "    # Continuous columns are deep, have a complex relationship with the output\n",
    "    deep = [mother_age,\n",
    "            gestation_weeks,\n",
    "            embed]\n",
    "    return wide, deep\n",
    "\n",
    "# Create serving input function to be able to serve predictions later using provided inputs\n",
    "def serving_input_fn():\n",
    "    feature_placeholders = {\n",
    "        'is_male': tf.placeholder(tf.string, [None]),\n",
    "        'mother_age': tf.placeholder(tf.float32, [None]),\n",
    "        'plurality': tf.placeholder(tf.string, [None]),\n",
    "        'gestation_weeks': tf.placeholder(tf.float32, [None]),\n",
    "        KEY_COLUMN: tf.placeholder_with_default(tf.constant(['nokey']), [None])\n",
    "    }\n",
    "    features = {\n",
    "        key: tf.expand_dims(tensor, -1)\n",
    "        for key, tensor in feature_placeholders.items()\n",
    "    }\n",
    "    return tf.estimator.export.ServingInputReceiver(features, feature_placeholders)\n",
    "\n",
    "# create metric for hyperparameter tuning\n",
    "def my_rmse(labels, predictions):\n",
    "    pred_values = predictions['predictions']\n",
    "    return {'rmse': tf.metrics.root_mean_squared_error(labels, pred_values)}\n",
    "\n",
    "def forward_features(estimator, key):\n",
    "    def new_model_fn(features, labels, mode, config):\n",
    "        spec = estimator.model_fn(features, labels, mode, config)\n",
    "        predictions = spec.predictions\n",
    "        predictions[key] = features[key]\n",
    "        spec = spec._replace(predictions=predictions)\n",
    "        return spec\n",
    "    return tf.estimator.Estimator(model_fn=new_model_fn, model_dir=estimator.model_dir, config=estimator.config)\n",
    "\n",
    "# Create estimator to train and evaluate\n",
    "def train_and_evaluate(output_dir):\n",
    "    tf.summary.FileWriterCache.clear() # ensure filewriter cache is clear for TensorBoard events file\n",
    "    wide, deep = get_wide_deep()\n",
    "    EVAL_INTERVAL = 300 # seconds\n",
    "\n",
    "    ## TODO 2a: set the save_checkpoints_secs to the EVAL_INTERVAL\n",
    "    run_config = tf.estimator.RunConfig(save_checkpoints_secs = EVAL_INTERVAL,\n",
    "                                        keep_checkpoint_max = 3)\n",
    "    \n",
    "    ## TODO 2b: change the dnn_hidden_units to NNSIZE\n",
    "    estimator = tf.estimator.DNNLinearCombinedRegressor(\n",
    "        model_dir = output_dir,\n",
    "        linear_feature_columns = wide,\n",
    "        dnn_feature_columns = deep,\n",
    "        dnn_hidden_units = NNSIZE,\n",
    "        config = run_config)\n",
    "    \n",
    "    # illustrates how to add an extra metric\n",
    "    estimator = tf.estimator.add_metrics(estimator, my_rmse)\n",
    "    # for batch prediction, you need a key associated with each instance\n",
    "    estimator = forward_features(estimator, KEY_COLUMN)\n",
    "    \n",
    "    ## TODO 2c: Set the third argument of read_dataset to BATCH_SIZE \n",
    "    ## TODO 2d: and set max_steps to TRAIN_STEPS\n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn = read_dataset('train', tf.estimator.ModeKeys.TRAIN, BATCH_SIZE),\n",
    "        max_steps = TRAIN_STEPS)\n",
    "    \n",
    "    exporter = tf.estimator.LatestExporter('exporter', serving_input_fn, exports_to_keep=None)\n",
    "    \n",
    "    ## TODO 2e: Lastly, set steps equal to EVAL_STEPS\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn = read_dataset('eval', tf.estimator.ModeKeys.EVAL, 2**15),  # no need to batch in eval\n",
    "        steps = EVAL_STEPS,\n",
    "        start_delay_secs = 60, # start evaluating after N seconds\n",
    "        throttle_secs = EVAL_INTERVAL,  # evaluate every N seconds\n",
    "        exporters = exporter)\n",
    "    \n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
    "    \n",
    "    estimator_base_path = os.path.join(output_dir, 'exporter')\n",
    "    estimator_path = estimator.export_saved_model(estimator_base_path, serving_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Task 3\n",
    "\n",
    "Once the code works in standalone mode, you can run it on Cloud AI Platform. Because this is on the entire dataset, it will take a while. The training run took about <b> 6 min </b> for me. You can monitor the job from the GCP console in the Cloud AI Platform section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://trainings-ml-deployment-appengine-ag/babyweight/trained_model europe-west1 babyweight_200629_181843\n",
      "jobId: babyweight_200629_181843\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing gs://trainings-ml-deployment-appengine-ag/babyweight/trained_model/#1593451327257869...\n",
      "Removing gs://trainings-ml-deployment-appengine-ag/babyweight/trained_model/events.out.tfevents.1593451327.gke-cml-0629-170321-6f3d-n1-highcpu-8-137566d6-1z0l#1593451370755355...\n",
      "Removing gs://trainings-ml-deployment-appengine-ag/babyweight/trained_model/exporter/#1593451367744740...\n",
      "Removing gs://trainings-ml-deployment-appengine-ag/babyweight/trained_model/checkpoint#1593451370099761...\n",
      "Removing gs://trainings-ml-deployment-appengine-ag/babyweight/trained_model/exporter/1593451367/variables/variables.data-00000-of-00002#1593451372311339...\n",
      "Removing gs://trainings-ml-deployment-appengine-ag/babyweight/trained_model/exporter/1593451367/variables/variables.data-00001-of-00002#1593451372396889...\n",
      "Removing gs://trainings-ml-deployment-appengine-ag/babyweight/trained_model/exporter/1593451370/variables/#1593451375500037...\n",
      "Removing gs://trainings-ml-deployment-appengine-ag/babyweight/trained_model/exporter/1593451370/#1593451375299827...\n",
      "Removing gs://trainings-ml-deployment-appengine-ag/babyweight/trained_model/exporter/1593451367/saved_model.pb#1593451372124954...\n",
      "Removing gs://trainings-ml-deployment-appengine-ag/babyweight/trained_model/exporter/1593451367/#1593451371684086...\n",
      "Removing gs://trainings-ml-deployment-appengine-ag/babyweight/trained_model/exporter/1593451370/variables/variables.index#1593451375798439...\n",
      "Removing gs://trainings-ml-deployment-appengine-ag/babyweight/trained_model/exporter/1593451367/variables/#1593451372220123...\n",
      "Removing gs://trainings-ml-deployment-appengine-ag/babyweight/trained_model/exporter/1593451370/saved_model.pb#1593451375388335...\n",
      "Removing gs://trainings-ml-deployment-appengine-ag/babyweight/trained_model/exporter/1593451370/variables/variables.data-00001-of-00002#1593451375679949...\n",
      "Removing gs://trainings-ml-deployment-appengine-ag/babyweight/trained_model/model.ckpt-0.data-00000-of-00004#1593451332053277...\n",
      "Removing gs://trainings-ml-deployment-appengine-ag/babyweight/trained_model/exporter/1593451367/variables/variables.index#1593451372506965...\n",
      "Removing gs://trainings-ml-deployment-appengine-ag/babyweight/trained_model/graph.pbtxt#1593451329199533...\n",
      "Removing gs://trainings-ml-deployment-appengine-ag/babyweight/trained_model/model.ckpt-0.data-00001-of-00004#1593451332237253...\n",
      "Removing gs://trainings-ml-deployment-appengine-ag/babyweight/trained_model/exporter/1593451370/variables/variables.data-00000-of-00002#1593451375588529...\n",
      "Removing gs://trainings-ml-deployment-appengine-ag/babyweight/trained_model/model.ckpt-0.data-00002-of-00004#1593451331863660...\n",
      "Removing gs://trainings-ml-deployment-appengine-ag/babyweight/trained_model/model.ckpt-0.data-00003-of-00004#1593451331669575...\n",
      "Removing gs://trainings-ml-deployment-appengine-ag/babyweight/trained_model/model.ckpt-0.index#1593451332408907...\n",
      "Removing gs://trainings-ml-deployment-appengine-ag/babyweight/trained_model/model.ckpt-0.meta#1593451333493308...\n",
      "Removing gs://trainings-ml-deployment-appengine-ag/babyweight/trained_model/model.ckpt-3914.data-00000-of-00004#1593451369250717...\n",
      "Removing gs://trainings-ml-deployment-appengine-ag/babyweight/trained_model/model.ckpt-3914.data-00001-of-00004#1593451369414155...\n",
      "Removing gs://trainings-ml-deployment-appengine-ag/babyweight/trained_model/model.ckpt-3914.data-00002-of-00004#1593451369095641...\n",
      "Removing gs://trainings-ml-deployment-appengine-ag/babyweight/trained_model/model.ckpt-3914.data-00003-of-00004#1593451368936461...\n",
      "Removing gs://trainings-ml-deployment-appengine-ag/babyweight/trained_model/model.ckpt-3914.index#1593451369552208...\n",
      "Removing gs://trainings-ml-deployment-appengine-ag/babyweight/trained_model/model.ckpt-3914.meta#1593451370554583...\n",
      "/ [29/29 objects] 100% Done                                                     \n",
      "Operation completed over 29 objects.                                             \n",
      "Job [babyweight_200629_181843] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe babyweight_200629_181843\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs babyweight_200629_181843\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/babyweight/trained_model\n",
    "JOBNAME=babyweight_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ai-platform jobs submit training $JOBNAME \\\n",
    "  --region=$REGION \\\n",
    "  --module-name=trainer.task \\\n",
    "  --package-path=$(pwd)/model/trainer \\\n",
    "  --job-dir=$OUTDIR \\\n",
    "  --staging-bucket=gs://$BUCKET \\\n",
    "  --scale-tier=STANDARD_1 \\\n",
    "  --runtime-version=2.1 \\\n",
    "  --python-version=3.7 \\\n",
    "  -- \\\n",
    "  --bucket=${BUCKET} \\\n",
    "  --output_dir=${OUTDIR} \\\n",
    "  --train_examples=25000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I ran it, I used train_examples=20000. When training finished, I filtered in the Stackdriver log on the word \"dict\" and saw that the last line was:\n",
    "<pre>\n",
    "Saving dict for global step 5714290: average_loss = 1.06473, global_step = 5714290, loss = 34882.4, rmse = 1.03186\n",
    "</pre>\n",
    "The final RMSE was 1.03 pounds."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m49",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
